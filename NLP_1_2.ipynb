{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our data, 4 documents\n",
    "\n",
    "docs=[\"NLP consists of computational tools of textual analysis\",\n",
    "      \"Pizza is a savory dish of Italian origin and the ingredients usually include cheese.\",\n",
    "      \"leafy branches are swaying in the wind and leaves are gently falling in the river.\",\n",
    "      \"The main task of machine learning is using computational methods to do prediction.\"\n",
    "     ]\n",
    "\n",
    "titles=[\"NLP\",\"Pizza\",\"tree\",\"ML\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similarities\n",
    "\n",
    "Which of the last three dosuments is more similar to the first doument?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CountVectorizer is a tool that count words inside each string.\n",
    "#At this point we are ignoring the order of words,\n",
    "#and the relationship between words.\n",
    "\n",
    "#bag of words\n",
    "bower=sklearn.feature_extraction.text.CountVectorizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docterm=bower.fit_transform(docs) #you can also do it in two steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bower. #explore this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of all the words in the corpus (aggregation of all documents)\n",
    "vocab=bower.vocabulary_\n",
    "print(len(vocab))\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bower.get_feature_names() \n",
    "#without building vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docterm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "docterm is a sparse matrix:\n",
    "\n",
    "- majority of the elements in the matrix are zero.\n",
    "\n",
    "- only non-zero elements are stored to help with efficiency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docterm)\n",
    "#rows => documents, columns=> words/terms/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docterm.toarray())   #non-sparse format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docterm.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dataframe based on our de-sparsed matrix\n",
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray(),\n",
    "                         columns=bower.get_feature_names(),\n",
    "                         index=titles )\n",
    "doctermDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trasnpose the matrix (swap rows with columns)\n",
    "doctermDF = pd.DataFrame(docterm.toarray().T,\n",
    "                         columns= titles,\n",
    "                         index=bower.get_feature_names())\n",
    "doctermDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Words (rows) with more than one non-zero element are present in multiple documents and can be used to measure similraity.\n",
    "\n",
    "- Word compisition of each document is available in each column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not see any single-letter words in our dataframe. All the single character tokens are ignored by the default tokenizer. \n",
    "\n",
    "If you want single character tokens to be in the vocabulary, then you have to use a costume tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.feature_extraction.text.CountVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the single-letter words...\n",
    "\n",
    "bower=sklearn.feature_extraction.text.CountVectorizer(\n",
    "    tokenizer=lambda txt: txt.split())\n",
    "docterm=bower.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doctermDF = pd.DataFrame(docterm.toarray().T,\n",
    "                         columns= titles,\n",
    "                        index=bower.get_feature_names())\n",
    "\n",
    "\n",
    "doctermDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the loss of the syntactic structure when we only use the frequency of words in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bower=sklearn.feature_extraction.text.CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "docterm=bower.fit_transform(docs)\n",
    "\n",
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray(),\n",
    "                         columns=bower.get_feature_names(),\n",
    "                         index=titles)\n",
    "doctermDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "df=pd.DataFrame(normalize(doctermDF),\n",
    "                columns=bower.get_feature_names(),\n",
    "                index=titles)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.277350*0.277350)*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's make the first document our reference\n",
    "ref=df.iloc[0]   #iloc gvies numerical location of a dataframe\n",
    "cosines=df.dot(ref)\n",
    "cosines.nlargest()   #sorted version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why do you think pizza document is somewhat similar to NLP document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we want to remove stop words, such as \"a\", \"the\", \"of\", etc. and repeat our analysis.\n",
    "\n",
    "- In some applications stop words usage might be insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bower=sklearn.feature_extraction.text.CountVectorizer(stop_words=\"english\")\n",
    "docterm=bower.fit_transform(docs)\n",
    "\n",
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray(),columns=bower.get_feature_names(),index=titles)\n",
    "doctermDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are not in the dataframe anymore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(normalize(doctermDF),\n",
    "                columns=bower.get_feature_names(),\n",
    "                index=titles)\n",
    "ref=df.iloc[0]\n",
    "similarities=df.dot(ref)\n",
    "similarities.nlargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we add the word \"document\" to all documents. So we expect to see more similarities based on word count. But, are the documents really more similar?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[\"first document: NLP consists of computational tools of textual analysis\",\n",
    "      \"second document: Pizza is a savory dish of Italian origin and the ingredients usually include cheese.\",\n",
    "      \"third document: leafy branches are swaying in the wind and leaves are gently falling in the river.\",\n",
    "      \"fourth document: The main task of machine learning is using computational methods to do prediction.\"\n",
    "     ]\n",
    " \n",
    "titles=[\"NLP\",\"Pizza\",\"tree\",\"ML\"]\n",
    "\n",
    "\n",
    "bower=sklearn.feature_extraction.text.CountVectorizer(\n",
    "    stop_words=\"english\")\n",
    "docterm=bower.fit_transform(docs)\n",
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray().T,columns= titles,\n",
    "                        index=bower.get_feature_names())\n",
    "\n",
    "doctermDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bower=sklearn.feature_extraction.text.CountVectorizer(stop_words=\"english\")\n",
    "docterm=bower.fit_transform(docs)\n",
    "\n",
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray(),columns=bower.get_feature_names(),index=titles)\n",
    "df=pd.DataFrame(normalize(doctermDF),columns=bower.get_feature_names(),index=titles)\n",
    "ref=df.iloc[0]\n",
    "similarities=df.dot(ref)\n",
    "similarities.nlargest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to give higher weight to \"surprising\" words?\n",
    "\n",
    "- How to penalize the words that appear across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use TfidfVectorizer instead of CountVectorizer\n",
    "bower=sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    stop_words=\"english\")\n",
    "docterm=bower.fit_transform(docs)\n",
    "\n",
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray().T,\n",
    "                         columns= titles,\n",
    "                        index=bower.get_feature_names())\n",
    "\n",
    "doctermDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "doctermDF = pd.DataFrame(docterm.toarray(),\n",
    "                         columns=bower.get_feature_names(),\n",
    "                         index=titles)\n",
    "df=pd.DataFrame(normalize(doctermDF),\n",
    "                columns=bower.get_feature_names(),\n",
    "                index=titles)\n",
    "ref=df.iloc[0]\n",
    "similarities=df.dot(ref)\n",
    "similarities.nlargest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "- A widely used NLP platform used in Python. \n",
    "\n",
    "- It's usually used for generic purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample text, sourced from the wikipedia of poetry\n",
    "text=\"\"\"Poetry is a form of literature that \n",
    "          uses aesthetic and often rhythmic qualities of language—such as phonaesthetics,\n",
    "          sound symbolism, and metre—to evoke meanings in addition to, \n",
    "          or in place of, the prosaic ostensible meaning.\n",
    "          Poetry has a long history – dating back to prehistoric times with hunting poetry\n",
    "          in Africa, and to panegyric and elegiac court poetry of the empires of the Nile,\n",
    "          Niger, and Volta River valleys.\n",
    "          \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This text is reasonably clean, compared to user-generated text. There is no misspellings, informal spellings, etc. \n",
    "\n",
    "- The \"-...-\" phrase is challenging, and might be confused with a word that has a hyphen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the tokens\n",
    "\n",
    "nltk_tokenized = nltk.word_tokenize(text)\n",
    "print(nltk_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "- Reducing the words to their basic forms, by trimming the ends.\n",
    "\n",
    "- for example: teaching and teaches will be reduced to teach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#porter stemmer\n",
    "porter = nltk.PorterStemmer()\n",
    "porter_results = [porter.stem(t) for t in nltk_tokenized]\n",
    "print(porter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter.stem(\"teaches\") ,porter.stem(\"teaching\") ,porter.stem(\"teacher\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#snowball stemmer\n",
    "\n",
    "snowball = nltk.SnowballStemmer(\"english\")\n",
    "snowball_results = [snowball.stem(t) for t in nltk_tokenized]\n",
    "print(snowball_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "- Reducing the words, to their linguistic cores.\n",
    "\n",
    "- For example, qualities to quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatizer_results= [lemmatizer.lemmatize(t) for t in nltk_tokenized]\n",
    "print(lemmatizer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer.lemmatize(\"studies\"), snowball.stem(\"studies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizing based on the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokenized_sents = nltk.sent_tokenize(text)\n",
    "print(nltk_tokenized_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excluding stopwords\n",
    "from nltk.corpus import stopwords\n",
    "print(sorted(stopwords.words('english')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- check the stopwords in the package you use. \n",
    "\n",
    "- Based on applications you might want to remove some words from the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_noSW = [w for w in nltk_tokenized if w not in stopwords.words('english')]\n",
    "print(text_noSW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part of speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sentence_pos = nltk.pos_tag(nltk_tokenized)\n",
    "print(sentence_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "- More user friendly and more off-the-shelf style\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")  #English core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc= nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc.   #explore this object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(    [w.text for w in spacy_doc]     )  #tokenized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spacy could detect the hyphen phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(    [w.lemma_ for w in spacy_doc]     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare lemmatization in Spacy and NLTK.\n",
    "\n",
    "\n",
    "Notice \"use\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Recongnition\n",
    "\n",
    "- Automatically detecting locations, organization, etc. \n",
    "\n",
    "- The corpus used to feed and train the package algorithms (training body), affect the results of the algorithms.\n",
    "\n",
    "- sometimes, we have the option to choose the training body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents=spacy_doc.ents\n",
    "for x in ents:\n",
    "    print(x.text + \": \" + x.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=spacy_doc.sents\n",
    "for item in sents:\n",
    "    print(\"sentence:\", item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2=\"UMD is a public research university in College Park, Maryland.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_doc= nlp(sentence2)\n",
    "spacy_doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents=spacy_doc.ents\n",
    "for x in ents:\n",
    "    print(x.text + \": \" + x.label_)\n",
    "    \n",
    "#GPE: geo-political entity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textblob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc=textblob.TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tb_doc.words)  #couldn't specify the phrase inside hyphens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w.lemmatize() for w in tb_doc.words])  #notice \"us\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc.sentences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task of detecting sentences in our text was not challenging for any of the packages. For example, adding abbreviations that have dots make the sentence detection harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram\n",
    "\n",
    "- Higher order of organizations of words\n",
    "\n",
    "- How to capture 'College Park\" together and different from 'College' and 'Park'?\n",
    "\n",
    "- We can add ngrams to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### noun phrases\n",
    "\n",
    "- they can be 2grams or have more words.\n",
    "\n",
    "- The presence and absence of phrases in different documents can help us to find similarities between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tb_doc.noun_phrases)\n",
    "\n",
    "#notice 'uses aesthetic'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc.np_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tb_doc.pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentence_pos)  #NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "- What sentiment the text has and what is the intensity.\n",
    "\n",
    "- One step beyond counting the words towards the meaning and the emotional content of the text\n",
    "\n",
    "- The simplest sentiment analysis returns the valence of the text, and assigns a number to the text. \n",
    "\n",
    "- Textblob return two numbers.\n",
    "\n",
    "- Polarity can be  postitve or negative.\n",
    "\n",
    "- Subjectivity is between zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc.sentiment_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_doc.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"great\")\n",
    "x.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"good\")\n",
    "x.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"not great\")\n",
    "x.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"good thing\")\n",
    "x.sentiment  #adding thing doesn't change polarity and subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"not a good thing\")\n",
    "x.sentiment  #a does not affect anything. #not multiplies the polarity by -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"very\")\n",
    "x.sentiment  #on average very is used more in positiove sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"very good\")\n",
    "x.sentiment #very multiplies the subjectivity by 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=textblob.TextBlob(\"very great\")\n",
    "x.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I believe that this is a terrible sentence and a horrible example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sentiment_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I don't believe that this is a terrible sentence and certanily don't think it's a horrible example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment  #textblob fails to capture the negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I believe that this is not a terrible sentence and I think it's not a horrible example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment  #now, textblob captures the negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I believe that this is not a bad sentence and I think it's not a horrible example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment #replacing terrible with bad decreases subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I know that this is not a bad sentence and I think it's not a horrible example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I think this is an average sentence and might work as an example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb.sentiment_assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=\"I think this is an above-average sentence and might work as an example. \"\n",
    "tb=textblob.TextBlob(x)\n",
    "tb.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the sentiment anlysis is implemented?\n",
    "\n",
    "- Usually a large corpus is used to train the supervised machine learning problem.\n",
    "\n",
    "- The labels are usually are given manually (binary or score). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NRC-emotion\n",
    "\n",
    "- going beyond sentiment and detect emotions.\n",
    "\n",
    "- Free and public lexicon: \n",
    "https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install NRCLex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"I like that beautiful tree\"\n",
    "text_object = NRCLex(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_object.affect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"I don't believe that this is a terrible sentence \n",
    "    and certanily don't think it's a horrible example. \"\"\"\n",
    "text_object = NRCLex(text)\n",
    "text_object.affect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
